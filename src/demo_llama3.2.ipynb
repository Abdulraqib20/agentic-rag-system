{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% local Agentic AI RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agentic_rag/\n",
    "├── logs/\n",
    "│   └── config.log\n",
    "├── src/\n",
    "│   └── demo_llama3.2.ipynb\n",
    "└── config/\n",
    "    └── appconfig.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SERPER_API_KEY' from 'config.appconfig' (c:\\Users\\Dell\\Desktop\\Machine Learning\\Agentic AI\\agentic_rag\\src\\..\\config\\appconfig.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mappconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     GROQ_API_KEY,\n\u001b[0;32m      8\u001b[0m     FIRECRAWL_API_KEY,\n\u001b[0;32m      9\u001b[0m     MODEL,\n\u001b[0;32m     10\u001b[0m     QDRANT_API_KEY,\n\u001b[0;32m     11\u001b[0m     QDRANT_LOCATION,\n\u001b[0;32m     12\u001b[0m     SERPER_API_KEY\n\u001b[0;32m     13\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SERPER_API_KEY' from 'config.appconfig' (c:\\Users\\Dell\\Desktop\\Machine Learning\\Agentic AI\\agentic_rag\\src\\..\\config\\appconfig.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append(os.path.abspath(\"config\"))\n",
    "\n",
    "from config.appconfig import (\n",
    "    GROQ_API_KEY,\n",
    "    FIRECRAWL_API_KEY,\n",
    "    MODEL,\n",
    "    QDRANT_API_KEY,\n",
    "    QDRANT_LOCATION,\n",
    "    SERPER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:19:43,865 - config.appconfig - INFO - Configuration loaded successfully\n",
      "2025-02-21 15:19:43,872 - config.appconfig - INFO - Loaded configuration values:\n",
      "2025-02-21 15:19:43,876 - config.appconfig - INFO - GROQ_API_KEY: gs****************************************************RI\n",
      "2025-02-21 15:19:43,880 - config.appconfig - INFO - FIRECRAWL_API_KEY: fc*******************************53\n",
      "2025-02-21 15:19:43,884 - config.appconfig - INFO - MODEL: gr************************le\n",
      "2025-02-21 15:19:43,888 - config.appconfig - INFO - QDRANT_API_KEY: LG**************************************************yg\n",
      "2025-02-21 15:19:43,898 - config.appconfig - INFO - QDRANT_LOCATION: ht*****************33\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from config.appconfig import (\n",
    "    GROQ_API_KEY,\n",
    "    FIRECRAWL_API_KEY,\n",
    "    MODEL,\n",
    "    QDRANT_API_KEY,\n",
    "    QDRANT_LOCATION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(os.path.abspath(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Crew, Task, LLM, Process\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from src.tools.custom_tool import DocumentSearchTool\n",
    "from src.tools.custom_tool import FireCrawlWebSearchTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"llama-3.3-70b-versatile\"\n",
    "# MODEL = \"groq/llama-3.3-70b-versatile\"\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "def load_llm():\n",
    "    llm = ChatGroq(model=MODEL, api_key=GROQ_API_KEY, temperature=TEMPERATURE)\n",
    "    return llm\n",
    "\n",
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:38:28,791 - model2vec.hf_utils - INFO - Folder does not exist locally, attempting to use huggingface hub.\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\model2vec\\hf_utils.py:157: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\Dell\\\\.cache\\\\huggingface\\\\hub\\\\models--minishlab--potion-base-8M\\\\snapshots\\\\3f12147e8ef0407df4e55de29669f79c11c8e2de\\\\config.json' mode='r' encoding='utf-8'>\n",
      "  config = json.load(open(config_path))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "pdf_tool = DocumentSearchTool(file_path='../src/knowledge/00_LATEST_RAQIB_CV.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 11:23:26,774 - model2vec.hf_utils - INFO - Folder does not exist locally, attempting to use huggingface hub.\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\model2vec\\hf_utils.py:157: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\Dell\\\\.cache\\\\huggingface\\\\hub\\\\models--minishlab--potion-base-8M\\\\snapshots\\\\3f12147e8ef0407df4e55de29669f79c11c8e2de\\\\config.json' mode='r' encoding='utf-8'>\n",
      "  config = json.load(open(config_path))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\u001b[32m2025-02-21 11:23:34.115\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mfastembed.common.model_management\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m274\u001b[0m - \u001b[31m\u001b[1mCould not download model from HuggingFace: [WinError 1314] A required privilege is not held by the client: '..\\\\..\\\\blobs\\\\d24f8d0cfab6e6a99fca53c1143c7d780aeb210c' -> 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Temp\\\\fastembed_cache\\\\models--Qdrant--bge-small-en\\\\snapshots\\\\8791246cc2a79c7949a4dc0d4a018cbd7d024879\\\\config.json' Falling back to other sources.\u001b[0m\n",
      "100%|██████████| 77.7M/77.7M [02:06<00:00, 616kiB/s] \n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\fastembed\\common\\model_management.py:164: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(\n"
     ]
    }
   ],
   "source": [
    "pdf_tool = DocumentSearchTool(file_path='../src/knowledge/00_LATEST_RAQIB_CV.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the API key for FireCrawl in your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = FireCrawlWebSearchTool(api_key=FIRECRAWL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_agent = Agent(\n",
    "    role=\"\"\"Retrieve relevant information to answer the user query: {query}\"\"\",\n",
    "    goal=\"\"\"Retrieve the most relevant information from the available sources \n",
    "            for the user query: {query}, always try to use the pdf search tool first. \n",
    "            If you are not able to retrieve the information from the pdf search tool \n",
    "            then try to use the web search tool.\"\"\",\n",
    "    backstory=\"\"\"You're a meticulous analyst with a keen eye for detail. \n",
    "                You're known for your ability understand the user query: {query} \n",
    "                and retrieve knowlege from the most suitable knowledge base.\"\"\",\n",
    "    verbose=True,\n",
    "    tools=[\n",
    "        pdf_tool,\n",
    "        web_search_tool\n",
    "    ],\n",
    "    # llm=llm\n",
    ")\n",
    "\n",
    "response_synthesizer_agent = Agent(\n",
    "    role=\"\"\"Response synthesizer agent for the user query: {query}\"\"\",\n",
    "    goal=\"\"\"Synthesize the retrieved information into a concise and coherent response \n",
    "            based on the user query: {query}. If you are not able to retrieve the \n",
    "            information then respond with \"I'm sorry, I couldn't find the information \n",
    "            you're looking for.\"\"\",\n",
    "    backstory=\"\"\"You're a skilled communicator with a knack for turning complex \n",
    "                information into clear and concise responses.\"\"\",\n",
    "    verbose=True,\n",
    "    # llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_task = Task(\n",
    "    description=\"\"\"Retrieve the most relevant information from the available \n",
    "                   sources for the user query: {query}\"\"\",\n",
    "    expected_output=\"\"\"The most relevant information in form of text as retrieved\n",
    "                       from the sources.\"\"\",\n",
    "    agent=retriever_agent\n",
    ")\n",
    "\n",
    "response_task = Task(\n",
    "    description=\"\"\"Synthesize the final response for the user query: {query}\"\"\",\n",
    "    expected_output=\"\"\"A concise and coherent response based on the retrieved infromation\n",
    "                       from the right source for the user query: {query}. If you are not \n",
    "                       able to retrieve the information then respond with \n",
    "                       I'm sorry, I couldn't find the information you're looking for.\"\"\",\n",
    "    agent=response_synthesizer_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "\tagents=[retriever_agent, response_synthesizer_agent], \n",
    "\ttasks=[retrieval_task, response_task],\n",
    "\tprocess=Process.sequential,\n",
    "\tverbose=True,\n",
    "\t# process=Process.hierarchical, # In case you wanna use that instead https://docs.crewai.com/how-to/Hierarchical/\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kickoff Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m15:40:29 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-02-21 15:40:29,971 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetrieve relevant information to answer the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mRetrieve the most relevant information from the available \n",
      "                   sources for the user query: Who is the President of the United States\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:40:34,854 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:40:34 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-21 15:40:34,864 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\crewai-env\\Lib\\site-packages\\qdrant_client\\qdrant_fastembed.py:714: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  self.search(\n",
      "\u001b[92m15:40:35 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-02-21 15:40:35,003 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetrieve relevant information to answer the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: To find the most relevant information for the user query \"Who is the President of the United States\", I should first try to use the DocumentSearchTool to search for the query in the available documents. If I am not able to retrieve the information from the DocumentSearchTool, then I will use the FireCrawlWebSearchTool to search the web.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDocumentSearchTool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"President of the United States\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Vendease Africa\n",
      "\n",
      "May 2024 – Present\n",
      "Lekki, Lagos\n",
      "\n",
      "___\n",
      "Abdulraqib Omotosho\n",
      "\n",
      "+234 (702) 5965-922 | abdulraqibshakir03@gmail.com | LinkedIn | GitHub | Portfolio Website | Blog\n",
      "\n",
      "Education\n",
      "\n",
      "University of Ilorin\n",
      "\n",
      "___\n",
      "Bachelor of Engineering in Computer Engineering\n",
      "\n",
      "Ilorin, Nigeria\n",
      "Aug. 2021 – Present\n",
      "\n",
      "• Relevant Coursework: Data Structures, Assembly Language, Basic Algorithms, Operating Systems, Calculus,\n",
      "\n",
      "Linear Algebra, Probability & Stats\n",
      "\n",
      "• Clubs & Affiliations: Campus Lead, Data Science Network (DSN, University of Ilorin)\n",
      "\n",
      "Experience\n",
      "\n",
      "Machine Learning Engineer\n",
      "\n",
      "___\n",
      "\n",
      "Developer Tools & Version Control: Git, GitHub, Docker, FastAPI, Postman, PyCharm, VS Code, Jupyter,\n",
      "Scripting, MS Office (PowerPoint, Word, Excel)\n",
      "Soft Skills: Technical Communication, Cross-functional Collaboration, Leadership, Report Writing, Problem-Solving,\n",
      "Critical Thinking\n",
      "\n",
      "\n",
      "___\n",
      "Conversation-Based RAG Chatbot | Python, Llama-Index, Chroma, Streamlit\n",
      "\n",
      "• Designed and deployed a highly efficient Retrieval-Augmented Generation (RAG) chatbot with conversational\n",
      "\n",
      "memory capabilities, capable of handling diverse document types (PDF, PPTX, DOCX, TXT) for accurate\n",
      "information retrieval.\n",
      "\n",
      "• Integrated Chroma vector store for fast embedding storage and retrieval, enhancing performance in large-scale\n",
      "\n",
      "document searches.\n",
      "\n",
      "• Performed advanced prompt engineering and model experimentation, optimizing responses across various\n",
      "\n",
      "LLMs to ensure high-quality answers.\n",
      "\n",
      "• Structured codebase for collaboration, ensuring modularity, scalability, and ease of maintenance.\n",
      "\n",
      "AI-Powered Sentiment Analysis & Insights App | NLTK, ML, Generative AI, Transformers\n",
      "\n",
      "• Developed and deployed an AI-powered Sentiment Analysis web app with integrated Generative AI, enabling\n",
      "\n",
      "users to chat and uncover insights from student feedback for enhanced learning experiences.\n",
      "\n",
      "• Achieved 96% model accuracy, providing deeper understanding of student emotions and aiding lecturers in\n",
      "\n",
      "refining teaching strategies.\n",
      "\n",
      "• Integrated EDA and interactive visualizations, elevating data exploration and improving user engagement.\n",
      "\n",
      "WhatsApp Chats Analyzer with AI Chatbot | Python, NLP, Generative AI, Data Analysis & Visualization\n",
      "\n",
      "• Developed and deployed an AI-powered WhatsApp chat analysis app, featuring a chatbot that interacts with\n",
      "\n",
      "processed CSV data from uploaded chat files, enabling users to uncover insights easily.\n",
      "\n",
      "• Integrated advanced NLP algorithms, including emoji analysis and keyword search, to extract valuable\n",
      "\n",
      "insights from chat data.\n",
      "\n",
      "• Designed an interactive interface with dynamic visualizations, customizable charts, word clouds, and other\n",
      "\n",
      "plots, enhancing data exploration and user engagement.\n",
      "\n",
      "\fTechnical Skills\n",
      "\n",
      "Languages: Python, SQL, Java (familiar), C++ (familiar)\n",
      "Machine Learning & AI: Supervised & Unsupervised Learning, Gradient Boosting Trees, Natural Language\n",
      "Processing, Neural Networks, Deep Learning, Generative AI & LLMs\n",
      "Libraries & Frameworks: Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, Plotly, NLTK, Langchain, Llama-index,\n",
      "Keras, TensorFlow, PyTorch, etc.\n",
      "___\n",
      "• Spearheaded with a team of two the development of the company’s first-ever AI-powered chatbot, designed to\n",
      "assist the operations team in tracking and monitoring purchases, invoices, and orders within the e-procurement\n",
      "marketplace.\n",
      "\n",
      "• Pioneered the entire lifecycle of the chatbot project, from ideation to deployment, contributing to its successful\n",
      "\n",
      "launch and live production across internal teams.\n",
      "\n",
      "• Implemented advanced conversational AI frameworks and LLM (Large Language Model) integrations to optimize\n",
      "\n",
      "the chatbot’s performance, making it a critical tool in streamlining business operations.\n",
      "\n",
      "• Worked closely with cross-functional teams (operations, product, and engineering) to align the chatbot’s\n",
      "\n",
      "functionalities with real business needs and to ensure seamless integration into existing workflows.\n",
      "\n",
      "• Provided regular stakeholder presentations and technical documentation to communicate project progress, key\n",
      "\n",
      "metrics, and the strategic impact of AI and machine learning in driving operational efficiency.\n",
      "\n",
      "Data Scientist\n",
      "Hamoye AI\n",
      "\n",
      "May 2023 – August 2023\n",
      "Lagos\n",
      "\n",
      "• Led multiple data projects as the project lead during my Spring 2023 internship, where I effectively supervised,\n",
      "\n",
      "delegated tasks, and organized workflows, earning recognition as one of the highest-performing interns due to my\n",
      "strong leadership, strategic contributions, and project execution skills.\n",
      "\n",
      "Projects\n",
      "\n",
      "\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:40:38,435 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:40:38 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-21 15:40:38,436 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:40:42 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-02-21 15:40:42,868 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetrieve relevant information to answer the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mAI Personal Assistant for the Visually Impaired | 2021\n",
      "• Developed a sophisticated assistive technology prototype, leveraging cutting-edge AI, machine learning, and IoT\n",
      "technologies, to assist visually impaired users navigate real-world environments and access information with ease.\n",
      "Sentiment Analysis of Text Data Using Machine Learning | 2021\n",
      "• Explored text features of 2400+ text documents, applying the NLTK library for natural language processing, before\n",
      "leveraging supervised machine learning algorithms to develop a highly accurate sentiment analysis model, achieving\n",
      "82.4% predictive accuracy.\n",
      "Voice Controlled AI Personal Assistant for IoT Devices | 2020\n",
      "• Built an end-to-end voice-controlled AI assistant, utilizing IoT devices, AWS Lambda, and other cloud-based\n",
      "technologies to enable seamless smart home automation and control.\n",
      "About Me\n",
      "My name is Abdulraqib Omotosho, a Tech Innovator and Computer Engineering Undergraduate with a strong\n",
      "passion for AI, machine learning, and software development. I am eager to solve real-world problems with\n",
      "innovative AI-driven solutions, collaborating with like-minded individuals who are committed to making an impact\n",
      "in the tech space.\n",
      "Looking to work with esteemed organizations focused on harnessing AI’s potential to drive meaningful social and\n",
      "economic change, enhancing global prosperity and growth for generations to come.\n",
      "Thought: The search results using the DocumentSearchTool did not provide any relevant information regarding the President of the United States.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mFireCrawlWebSearchTool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"President of the United States\\\", \\\"max_results\\\": 10}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Error during FireCrawl web search: HTTPSConnectionPool(host='api.firecrawl.com', port=443): Max retries exceeded with url: /search?query=President+of+the+United+States&max_results=10 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002391F867BC0>: Failed to resolve 'api.firecrawl.com' ([Errno 11001] getaddrinfo failed)\"))\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:40:45,904 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:40:45 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-21 15:40:45,919 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m15:40:45 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n",
      "2025-02-21 15:40:45,981 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mRetrieve relevant information to answer the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "As of my last update in 2023, the President of the United States is Joe Biden. However, please note that this information may be outdated due to the limitations encountered during my search. For the most accurate and up-to-date information, I recommend checking a reliable news source or official government website.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResponse synthesizer agent for the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mSynthesize the final response for the user query: Who is the President of the United States\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:40:48,454 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m15:40:48 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-21 15:40:48,464 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResponse synthesizer agent for the user query: Who is the President of the United States\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "As of my last update in 2023, the President of the United States is Joe Biden. However, please note that this information may be outdated due to the limitations encountered during my search. For the most accurate and up-to-date information, I recommend checking a reliable news source or official government website.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff(inputs={\"query\": \"Who is the President of the United States\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in 2023, the President of the United States is Joe Biden. However, please note that this information may be outdated due to the limitations encountered during my search. For the most accurate and up-to-date information, I recommend checking a reliable news source or official government website.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
